### 💡 설계를 고민하며 작성한 문서입니다. 최종 구현에 반영된 내용은 아래 문서입니다.

**[최종 결정 내용](https://github.com/softeerbootcamp-7th/WEB-Team7-Jackpot/wiki/%5B%EC%9D%B4%EC%A0%95%EB%AF%BC%5D-pdf%EC%97%85%EB%A1%9C%EB%93%9C-%EB%B0%8F-AI-%EB%9D%BC%EB%B2%A8%EB%A7%81-%EA%B8%B0%EB%8A%A5%EC%9D%84-%EC%9C%84%ED%95%9C-%EC%9D%B4%EB%B2%A4%ED%8A%B8-%EA%B8%B0%EB%B0%98-%EB%B9%84%EB%8F%99%EA%B8%B0-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8)**


***


### 1. 문제

- **상황:** 사용자가 이력서 분석을 위해 Presigned URL을 발급받았으나, 실제 S3 업로드를 수행하지 않거나 네트워크 오류로 실패하는 경우가 발생함.
- **문제점:**
    - S3 업로드 이벤트가 발생하지 않으므로 Lambda 트리거가 작동하지 않음.
    - 결과적으로 SQS 메시지가 생성되지 않아, 서버는 해당 Job의 상태를 영원히 `PENDING`으로 유지함.
    - 사용자는 성공/실패 여부를 알지 못한 채 무한정 기다리게 되며, 시스템에는 '완료되지 않은' 데이터가 누적됨.
- **목표:** 업로드 시도가 일정 시간(10분) 내에 완료되지 않으면 **자동으로 실패 처리해야함**

`문제: 사용자가 Presigned URL만 받고 s3에 실제 업로드를 하지 않거나 실패할 경우, DB 상태가 영원히 PENDING으로 남음. 실패, 성공 처리가 되지 않아 알림 전송 불가. `

<br>

### 2. 해결 방안

**2.1. 클라이언트에서 업로드 완료 후 Job 생성**

 :  클라이언트가 S3 업로드를 성공적으로 마친 후 서버에 Job 생성을 요청한다. 이 방식은 업로드가 확인된 파일만 Job으로 등록하므로 불필요한 데이터를 생성하지 않아 로직이 직관적이라는 장점이 있다.
 그러나 신뢰성 측면에서 문제가 있다. 브라우저가 강제로 종료되거나 네트워크가 끊기는 등 클라이언트 환경에 문제가 발생하면, 서버는 업로드 시도조차 감지할 수 없다. 결국 서버가 정합성을 보장할 수 없는 수동적인 입장이 될 수 있다.

**2.2. Batch Scheduler**

**:** 주기적으로 DB를 조회하여 PENDING 상태인 Job 중 일정 시간 이상 경과한 경우 실패로 처리한다. 서버가 직접 Job을 관리하고 안정적이다. 다만 데이터가 쌓일수록 조회 쿼리의 부하가 커지며, 불필요한 쿼리가 지속적으로 발생한다.

**2.3. SQS Delayed Message**

**:** Job 생성 시점에 "10분 뒤에 확인해 달라"는 메시지를 SQS에 예약 전송하는 방식입니다. 10분 후 메시지를 수신한 서버는 해당 Job(ID)만 단건 조회하여, 여전히 `PENDING` 상태라면 실패로 간주하고 알림을 보낸다. 2번 방식보다 DB 부하가 적을 것이다. 큐를 하나 더 관리해야한다는 단점이 있다.

**2.4.  aws EventBridge Scheduler**

: AWS의 서버리스 스케줄러를 사용하여 특정 시점에 정확히 이벤트를 발생시키는 방식이다. 초기 인프라 구성 복잡도가 높아 지금 도입을 고려할 대상이 아니라고 판단했다.

<br>

### 3. 결론 도출 과정
**3.1. 업로드 실패 시 이후 로직이 실행되지 않는데, 굳이 선행적으로 Job을 생성해야할까?**

다음과 같은 이유로 Job을 먼저 생성해야한다고 판단했다.
1. **Observability** : 실패한 시도도 기록될 필요가 있다. 프로그램 동작 측면에서는 무의미한 데이터일지라도 운영을 고려하면 필요한 데이터일 수 있다. 예를 들면 presigned url 발급 이후 업로드 실패가 빈번한지 등의 문제를 분석하는데 활용될 수 있다.
2. **리소스 관리** : 클라이언트가 업로드만 하고 job을 생성하지 못하고 사라질 경우, Job 기록이 없으면 S3에 남겨진 고아 파일(Orphan Object)을 추적하여 삭제할 방법이 없다. 람다 트리거로 자동 실행되더라도 추후 DB에 들어왔을 때 생성되어 있는 job이 없음으로 처리될 수 없다. Job ID 기반으로 s3 key를 미리 매핑해두면 s3의 리소스를 관리할 수 있다.
3. **트랜잭션 주도권** : Presigned URL 발급 시점에 이미 서버는 요청을 인지한다. 이를 DB화하여 관리하는 것이 트랜잭션의 시작점을 서버가 통제하는 설계이다. 
<br>

**3.2. 업로드 실패를 하면 서버도 즉시 알게되는게 좋지 않을까?**

 Batch, SQS Delay 방식은 일정 시간이 경과한 후에야 상태를 변경하므로 한계가 있다. 클라이언트가 s3로 파일을 보내면 s3는 상태코드로 응답을 준다. 실패했다면, 서버에게 즉시 알려주면 된다!

<br>

### 4. 결론
- Presigned URL 발급 시 job_id 생성 후 함께 반환
- S3 업로드 실패 시: 프론트엔드가 서버에 실패 알림 → 상태를 FAILED로 즉시 처리
- S3 업로드 성공 시: Lambda 트리거 실행 → 정상 플로우 진행

장점 : 실패 즉시 감지 및 처리, 별도 스케쥴러 등 로직 없이 단순하고 직관적임
단점 : 엣지케이스가 존재한다. (업로드 도중 유저가 브라우저 탭을 닫아버려 실패처리 API를 호출하지 못한 경우?)

### 5. 추가사항
#### AWS S3 수명 주기(Lifecycle)와 연동

서버 로직과는 별개로, S3에 '업로드 중 중단된 파일'이 용량만 차지하는 것을 방지하기 위해 설정한다.
- **S3 설정**: 버킷의 [관리(Management)] 탭 -> [수명 주기 규칙] 생성.
- **규칙**: "완료되지 않은 멀티파트 업로드 중단" 설정을 켜두면, 업로드에 실패하고 남은 찌꺼기 파일들을 AWS가 알아서 지워준다.
